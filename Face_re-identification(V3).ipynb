{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import face_recognition\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MTCNN model for face detection\n",
    "mtcnn = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "video_path = \"Class Room Entrance.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Use \"mp4v\" for mp4 format\n",
    "output_path = \"output_video.mp4\"\n",
    "out = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_id = 0\n",
    "skip_frames = 2  # Skip the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size threshold for faces (adjust as needed)\n",
    "min_face_size = 10000  # For example, minimum area of 10000 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to track whether the face image and embedding have been saved for each ID\n",
    "saved_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save face images\n",
    "os.makedirs(\"faces(V3)\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_id += 1\n",
    "\n",
    "    # Skip frames until reaching the nth frame\n",
    "    if frame_id <= skip_frames:\n",
    "        continue\n",
    "\n",
    "    # Detect faces using MTCNN\n",
    "    faces = mtcnn.detect_faces(frame)\n",
    "    bbs = [(face['box'], face['confidence']) for face in faces] #, face['keypoints']\n",
    "    print(bbs)\n",
    "    # Update tracker with the detected faces\n",
    "    tracks = tracker.update_tracks(bbs, frame=frame)\n",
    "\n",
    "    # Draw bounding boxes and IDs on the frame\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "\n",
    "        # Check if data for this ID has already been saved\n",
    "        if track_id in saved_data:\n",
    "            continue\n",
    "\n",
    "        ltrb = track.to_ltrb()\n",
    "\n",
    "        # Check face size\n",
    "        face_area = (ltrb[2] - ltrb[0]) * (ltrb[3] - ltrb[1])\n",
    "        if face_area < min_face_size:\n",
    "            continue  # Skip small faces\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (int(ltrb[0]), int(ltrb[1])), (int(ltrb[2]), int(ltrb[3])), (0, 255, 0), 2)\n",
    "\n",
    "        # Get facial landmarks using face_recognition library\n",
    "        face_locations = [(int(ltrb[1]), int(ltrb[2]), int(ltrb[3]), int(ltrb[0]))]  # (top, right, bottom, left)\n",
    "        landmarks = face_recognition.face_landmarks(frame, face_locations)\n",
    "\n",
    "        if landmarks:\n",
    "            # Get face embedding\n",
    "            face_encoding = face_recognition.face_encodings(frame, face_locations)[0]\n",
    "\n",
    "            # Check if the face embedding is not empty before saving\n",
    "            if len(face_encoding) > 0:\n",
    "                # Compare with existing embeddings\n",
    "                match = False\n",
    "                for saved_id, saved_info in saved_data.items():\n",
    "                    saved_embedding = saved_info[\"embedding\"]\n",
    "                    # Compare the face embeddings using face_recognition library\n",
    "                    results = face_recognition.compare_faces([saved_embedding], face_encoding)\n",
    "\n",
    "                    # Check if there is a match\n",
    "                    if any(results):\n",
    "                        match = True\n",
    "                        break\n",
    "\n",
    "                # If no match, save the face embedding\n",
    "                if not match:\n",
    "                    # Save the face embedding\n",
    "                    saved_data[track_id] = {\n",
    "                        \"embedding\": face_encoding,\n",
    "                        \"image_path\": f\"faces(V3)/{track_id}_frame{frame_id}.png\"\n",
    "                    }\n",
    "\n",
    "                    # Get the face bounding box coordinates\n",
    "                    top, right, bottom, left = int(ltrb[1]), int(ltrb[2]), int(ltrb[3]), int(ltrb[0])\n",
    "\n",
    "                    # Check if the bounding box coordinates are within the frame boundaries\n",
    "                    if 0 <= top < frame.shape[0] and 0 <= left < frame.shape[1] and top < bottom and left < right:\n",
    "                        # Capture the face image\n",
    "                        face_image = frame[top:bottom, left:right]\n",
    "\n",
    "                        # Check if the face image is not empty before saving\n",
    "                        if not face_image.size == 0:\n",
    "                            # Save the face image\n",
    "                            cv2.imwrite(saved_data[track_id][\"image_path\"], face_image)\n",
    "\n",
    "                            # Draw label\n",
    "                            label = f\"ID: {track_id}\"\n",
    "                            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame with bounding boxes to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the age prediction model\n",
    "loaded_age_model = joblib.load('age_model.pkl')\n",
    "\n",
    "# Load the gender classification model\n",
    "loaded_gender_model = joblib.load('gender_model.pkl')\n",
    "\n",
    "# Lists to store predicted ages and genders\n",
    "age_array = []\n",
    "gender_array = []\n",
    "\n",
    "# Iterate through saved_data for age and gender prediction\n",
    "for track_id, info in saved_data.items():\n",
    "    # Get the face embedding\n",
    "    face_embedding = info[\"embedding\"]\n",
    "\n",
    "    # Reshape the face embedding to match the input shape expected by the models\n",
    "    face_embedding_reshaped = face_embedding.reshape(1, -1)\n",
    "    print(face_embedding_reshaped)\n",
    "    # Predict age\n",
    "    predicted_age = loaded_age_model.predict(face_embedding_reshaped)[0]\n",
    "    age_array.append(predicted_age)\n",
    "\n",
    "    # Predict gender\n",
    "    predicted_gender = loaded_gender_model.predict(face_embedding_reshaped)[0]\n",
    "    gender_array.append(predicted_gender)\n",
    "\n",
    "# Convert age_array and gender_array to numpy arrays\n",
    "age_array = np.array(age_array)\n",
    "gender_array = np.array(gender_array)\n",
    "\n",
    "# Print or use the predicted age and gender arrays as needed\n",
    "print(\"Predicted Ages:\", age_array)\n",
    "print(\"Predicted Genders:\", gender_array)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
