{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 182] The operating system cannot run %1. Error loading \"c:\\Users\\SAnuv\\anaconda3\\envs\\ahead\\lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[1;32mc:\\Users\\SAnuv\\anaconda3\\envs\\ahead\\lib\\site-packages\\torch\\__init__.py:128\u001b[0m\n\u001b[0;32m    126\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    127\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 182] The operating system cannot run %1. Error loading \"c:\\Users\\SAnuv\\anaconda3\\envs\\ahead\\lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import face_recognition\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSort(max_age=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MTCNN model for face detection\n",
    "mtcnn = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "video_path = \"Class Room Entrance.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Use \"mp4v\" for mp4 format\n",
    "output_path = \"output_video.mp4\"\n",
    "out = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_id = 0\n",
    "skip_frames = 2  # Skip the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size threshold for faces (adjust as needed)\n",
    "min_face_size = 10000  # For example, minimum area of 10000 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to track whether the face image and embedding have been saved for each ID\n",
    "saved_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save face images\n",
    "os.makedirs(\"faces(V3)\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_id += 1\n",
    "\n",
    "    # Skip frames until reaching the nth frame\n",
    "    if frame_id <= skip_frames:\n",
    "        continue\n",
    "\n",
    "    # Detect faces using MTCNN\n",
    "    faces = mtcnn.detect_faces(frame)\n",
    "    bbs = [(face['box'], face['confidence'], face['keypoints']) for face in faces]\n",
    "\n",
    "    # Update tracker with the detected faces\n",
    "    tracks = tracker.update_tracks(bbs, frame=frame)\n",
    "\n",
    "    # Draw bounding boxes and IDs on the frame\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "\n",
    "        # Check if data for this ID has already been saved\n",
    "        if track_id in saved_data:\n",
    "            continue\n",
    "\n",
    "        ltrb = track.to_ltrb()\n",
    "\n",
    "        # Check face size\n",
    "        face_area = (ltrb[2] - ltrb[0]) * (ltrb[3] - ltrb[1])\n",
    "        if face_area < min_face_size:\n",
    "            continue  # Skip small faces\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (int(ltrb[0]), int(ltrb[1])), (int(ltrb[2]), int(ltrb[3])), (0, 255, 0), 2)\n",
    "\n",
    "        # Get facial landmarks using face_recognition library\n",
    "        face_locations = [(int(ltrb[1]), int(ltrb[2]), int(ltrb[3]), int(ltrb[0]))]  # (top, right, bottom, left)\n",
    "        landmarks = face_recognition.face_landmarks(frame, face_locations)\n",
    "\n",
    "        if landmarks:\n",
    "            # Get face embedding\n",
    "            face_encoding = face_recognition.face_encodings(frame, face_locations)[0]\n",
    "\n",
    "            # Check if the face embedding is not empty before saving\n",
    "            if len(face_encoding) > 0:\n",
    "                # Compare with existing embeddings\n",
    "                match = False\n",
    "                for saved_id, saved_info in saved_data.items():\n",
    "                    saved_embedding = saved_info[\"embedding\"]\n",
    "                    # Compare the face embeddings using face_recognition library\n",
    "                    results = face_recognition.compare_faces([saved_embedding], face_encoding)\n",
    "\n",
    "                    # Check if there is a match\n",
    "                    if any(results):\n",
    "                        match = True\n",
    "                        break\n",
    "\n",
    "                # If no match, save the face embedding\n",
    "                if not match:\n",
    "                    # Save the face embedding\n",
    "                    saved_data[track_id] = {\n",
    "                        \"embedding\": face_encoding,\n",
    "                        \"image_path\": f\"faces(V3)/{track_id}_frame{frame_id}.png\"\n",
    "                    }\n",
    "\n",
    "                    # Get the face bounding box coordinates\n",
    "                    top, right, bottom, left = int(ltrb[1]), int(ltrb[2]), int(ltrb[3]), int(ltrb[0])\n",
    "\n",
    "                    # Check if the bounding box coordinates are within the frame boundaries\n",
    "                    if 0 <= top < frame.shape[0] and 0 <= left < frame.shape[1] and top < bottom and left < right:\n",
    "                        # Capture the face image\n",
    "                        face_image = frame[top:bottom, left:right]\n",
    "\n",
    "                        # Check if the face image is not empty before saving\n",
    "                        if not face_image.size == 0:\n",
    "                            # Save the face image\n",
    "                            cv2.imwrite(saved_data[track_id][\"image_path\"], face_image)\n",
    "\n",
    "                            # Draw label\n",
    "                            label = f\"ID: {track_id}\"\n",
    "                            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame with bounding boxes to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
